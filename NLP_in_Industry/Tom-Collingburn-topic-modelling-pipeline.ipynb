{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Topic modelling pipeline using spaCy, Gensim and LDAVis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#set working directory\n",
    "os.chdir('C:/your_working_directory')\n",
    "\n",
    "#read the original excel supplied by service department\n",
    "import pandas as pd\n",
    "raw_df = pd.read_excel('your_raw_input.xlsx')\n",
    "\n",
    "#if you just have on import field and not additional criteria you can cut down this code\n",
    "cleaned_col = raw_df['field_to_import'][raw_df['filtered_by_some_column']!='Excluding_this_criteria'].replace(r'\\r|\\n|\\\\j', '', regex = True).fillna('')\n",
    "\n",
    "#\\n is the new line character and is used to separate responses as they are separated per cell in the excel\n",
    "text = '\\n'.join(cleaned_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#what does the raw text look like?\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing with spaCy\n",
    "\n",
    "Assuming an understanding of the standard techniques for preparing raw text for analysis I am going to use Spacy instead of these established processes to really show off a small portion Spacy's true power.\n",
    "If you want to get really deep with Spacy take a look at their website https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ask spacy to parse the raw text into a new variable\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now how does the text look once parsed by spacy?\n",
    "doc[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's pretty impressive Spacy has parsed our raw text and just in the preview above you can see how its added new lines for full stops and split out each call description as it's own document.\n",
    "But there's a whole lot more that you can't see there, so let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spacy tags every word with various metrics such as it's POS (Parts of speech) which is essentially the type of word.\n",
    "#the base form of the word (lemmatised) much better than stemming which is brutal and inaccurate by comparison\n",
    "#TAG The detailed part-of-speech tag. - unique to Spacy\n",
    "#DEP : Syntactic dependency, i.e. the relation between tokens.\n",
    "for token in doc[2:20]: #just the first 20 words\n",
    "    print(token.text\n",
    "         ,token.lemma_\n",
    "         ,token.pos_\n",
    "         ,token.tag_\n",
    "         ,token.dep_\n",
    "         ,token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for checking the various named entity types which we may chose to exclude later\n",
    "for ent in doc.ents[23:25]:\n",
    "    print(ent.text\n",
    "          #, ent.start_char\n",
    "          #, ent.end_char\n",
    "          , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#structure our newly parsed text using only the content we are interested in\n",
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if (w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and not w.like_email\n",
    "        and not w.like_url \n",
    "        #we are only interested in certain entity types so we're excluding these\n",
    "        and w.ent_type_ not in ['PERSON','CARDINAL','MONEY'\n",
    "                                ,'TIME','DATE','GPE','LOC'\n",
    "                                ,'MONEY','QUANTITIY']):\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Spacy has allowed us to remove words by their type so we just keep what we want to analyse, and it's very easy to go back and make changes to the process should you want to include something which you previously excluded. It's non destructive which is very handy for working iteratively which I do a lot in text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenSim - Identifying unigrams and topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#when playing with the parameters you need to reload \"texts\" from 2 cells above, otherwise you will be further processing\n",
    "#the already processed \"texts\" from the previous run of this cell\n",
    "unigram = gensim.models.Phrases(texts, min_count=3, threshold=0.1)\n",
    "\n",
    "texts = [unigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at some of the unigrams identifed by GenSim's phrase detection.\n",
    "By analysing the list below then tweaking the phrase variables one can iteratively tune to get the most sensible mix of unigrams.\n",
    "The unigrams are joined by and underscore, a human will read these as a 2 word phrase but to the modelling in the next step they will be represented by just a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[5:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw text reduced down to just the words and phrases which we feel gives the documents meaning there is one more step before we can begin to model the data.\n",
    "Given that we want to leverage various statistical techniques to help us identify meaningful patterns in our text data we need to get the text into a useable numeric format.\n",
    "Here we convert the text into a dictionary and a corpus (explanations below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary simply lists each word with its own numerical index.\n",
    "We can inspect individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus holds the document structure in a simple list of coordinates where each word or phrase is held within () the first number being the word ID which corresponds to the dictionary, the second number being the frequency of that word within the document. The multiple words for each document are contained within []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to numerically represent text data such as sparse matricies which allow one to do useful analyses such as Term Frequency - Inverse Document Frequency (see here https://en.wikipedia.org/wiki/Tf%E2%80%93idf )\n",
    "Since I'm interested in topic modelling here I don't need to do such a thing, also GenSim expects the data in the format we have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW FOR THE TOPIC MODELS\n",
    "### LDA\n",
    "\n",
    "Latent Dirichlet Allocation is a tried and tested technique used for topic modelling, adapted from Dirichlet Distributions specifically for the purpose of identifying topics in large volumes of text.\n",
    "further reading here: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "and here: https://en.wikipedia.org/wiki/Dirichlet_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#be sure to check out gensim's other models, I had sucess with LSI also.\n",
    "#start high with your clusters and sense check the results, gradually\n",
    "#reducing the number of clusters until you have fewer but more concise clusters.\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=8, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis - sharing the results\n",
    "\n",
    "As you can see above, representing the topics with just words and a nmumber showing the contribution of each word to the topic isn't exactly an intuitive way of exploring the result even for an analyst, let alone sharing the results back to someone who has the domain knowledge but is most likely not an analytical p`erson.\n",
    "As with all types of analyses, good visualisation is required to get the most from your results.\n",
    "Here I use PyLDAvis which is a tool for graphically representing and interacting with LDA model outputs.\n",
    "More info here: https://pypi.org/project/pyLDAvis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
